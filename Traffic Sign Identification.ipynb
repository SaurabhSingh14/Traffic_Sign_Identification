{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc0c704",
   "metadata": {},
   "source": [
    "The aim of this project is to classify correct traffic signs by looking at the image. The dataset is taken from kaggle and the core implementation is done using Convolutional Neural Network.\n",
    "\n",
    "The dataset has 43 different classes that are required to predict, one at a time, by providing corresponding images to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eb8383",
   "metadata": {},
   "source": [
    "### Importing the standard libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef7b092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3acce636",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10672717",
   "metadata": {},
   "source": [
    "### Training images data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3362de3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 43 #Total number of classes available to classify\n",
    "data = [] #To hold image's data in the form of arrays after resizing.\n",
    "labels = [] #To hold image's corresponding label\n",
    "for i in range(classes):\n",
    "    img_path = os.path.join('data\\Train', str(i)) #Setting up the path\n",
    "    for img in os.listdir(img_path): #Iterating through each image\n",
    "        im = Image.open(img_path+'/'+img)\n",
    "        im = im.resize((30,30)) #Resizing the image\n",
    "        data.append(np.array(im)) #appending image's array data inside data list defined above\n",
    "        labels.append(i) #Appending labels\n",
    "\n",
    "data = np.array(data) #converting data of list type into ndimensional array type\n",
    "labels = np.array(labels) #Doing the same as above with labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7041d760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img = Image.open(r'data\\Train\\0\\00000_00001_00000.png') #Opening the image in order to show after reducing its size.\n",
    "img = img.resize((30, 30)) #Reducing the size of image\n",
    "sr = np.array(img) \n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a86993",
   "metadata": {},
   "source": [
    "### Splitting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "661580d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape:  (31367, 30, 30, 3) (31367,)\n",
      "testing shape:  (7842, 30, 30, 3) (7842,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Splitting the training data into 80-20%\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "print(\"training shape: \",x_train.shape, y_train.shape)\n",
    "print(\"testing shape: \",x_test.shape, y_test.shape)\n",
    "y_train = to_categorical(y_train, 43)\n",
    "y_test = to_categorical(y_test, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f30175",
   "metadata": {},
   "source": [
    "### Building the architecture of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53a22b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = tf.keras.models.Sequential()\n",
    "#Creating convolutional layer with 32 different feature detectors of size 5X5\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 5, activation = \"relu\", input_shape = x_train.shape[1:]))\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 32, kernel_size = 5, activation = \"relu\"))\n",
    "#Creating pooling layer of size 2X2 to keep spatial invariance\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2))\n",
    "#To avoid overfitting\n",
    "cnn.add(tf.keras.layers.Dropout(rate=0.25))\n",
    "\n",
    "#Repeating the same procedure to make another set of Convolutional and Pooling layer\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = \"relu\"))\n",
    "cnn.add(tf.keras.layers.Conv2D(filters = 64, kernel_size = 3, activation = \"relu\"))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size = 2))\n",
    "cnn.add(tf.keras.layers.Dropout(rate=0.25))\n",
    "\n",
    "#Flattening the map values to give input to the neural network.\n",
    "cnn.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#Making hidden layer of neuron 256 units\n",
    "cnn.add(tf.keras.layers.Dense(units = 256, activation = 'relu'))\n",
    "cnn.add(tf.keras.layers.Dropout(rate=0.5))\n",
    "\n",
    "#Creating output layer for 43 different classes\n",
    "cnn.add(tf.keras.layers.Dense(units = 43, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1ea262",
   "metadata": {},
   "source": [
    "### Intializing and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5d0c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compiling with adam optimizer to implement stochastic technique to backpropagate the error.\n",
    "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb7829d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "491/491 [==============================] - 101s 206ms/step - loss: 2.4347 - accuracy: 0.3972 - val_loss: 0.8044 - val_accuracy: 0.8196\n",
      "Epoch 2/15\n",
      "491/491 [==============================] - 100s 205ms/step - loss: 0.7074 - accuracy: 0.7941 - val_loss: 0.1880 - val_accuracy: 0.9592\n",
      "Epoch 3/15\n",
      "491/491 [==============================] - 100s 204ms/step - loss: 0.4020 - accuracy: 0.8806 - val_loss: 0.1028 - val_accuracy: 0.9744\n",
      "Epoch 4/15\n",
      "491/491 [==============================] - 101s 206ms/step - loss: 0.2871 - accuracy: 0.9179 - val_loss: 0.0716 - val_accuracy: 0.9814\n",
      "Epoch 5/15\n",
      "491/491 [==============================] - 101s 205ms/step - loss: 0.2220 - accuracy: 0.9339 - val_loss: 0.0623 - val_accuracy: 0.9847\n",
      "Epoch 6/15\n",
      "491/491 [==============================] - 101s 206ms/step - loss: 0.2032 - accuracy: 0.9406 - val_loss: 0.0644 - val_accuracy: 0.9813\n",
      "Epoch 7/15\n",
      "491/491 [==============================] - 101s 205ms/step - loss: 0.1795 - accuracy: 0.9481 - val_loss: 0.0417 - val_accuracy: 0.9898\n",
      "Epoch 8/15\n",
      "491/491 [==============================] - 101s 205ms/step - loss: 0.1759 - accuracy: 0.9502 - val_loss: 0.0370 - val_accuracy: 0.9916\n",
      "Epoch 9/15\n",
      "491/491 [==============================] - 101s 205ms/step - loss: 0.1555 - accuracy: 0.9557 - val_loss: 0.0321 - val_accuracy: 0.9917\n",
      "Epoch 10/15\n",
      "491/491 [==============================] - 102s 208ms/step - loss: 0.1453 - accuracy: 0.9590 - val_loss: 0.0464 - val_accuracy: 0.9898\n",
      "Epoch 11/15\n",
      "491/491 [==============================] - 101s 206ms/step - loss: 0.1386 - accuracy: 0.9608 - val_loss: 0.0485 - val_accuracy: 0.9894\n",
      "Epoch 12/15\n",
      "491/491 [==============================] - 101s 205ms/step - loss: 0.1658 - accuracy: 0.9535 - val_loss: 0.0338 - val_accuracy: 0.9894\n",
      "Epoch 13/15\n",
      "491/491 [==============================] - 101s 205ms/step - loss: 0.1366 - accuracy: 0.9631 - val_loss: 0.0319 - val_accuracy: 0.9921\n",
      "Epoch 14/15\n",
      "491/491 [==============================] - 105s 213ms/step - loss: 0.1462 - accuracy: 0.9609 - val_loss: 0.0311 - val_accuracy: 0.9929\n",
      "Epoch 15/15\n",
      "491/491 [==============================] - 104s 213ms/step - loss: 0.1354 - accuracy: 0.9636 - val_loss: 0.0320 - val_accuracy: 0.9920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fbaae90c40>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training the model for total epochs of 15 having batch size of 64 images data.\n",
    "cnn.fit(x_train, y_train, validation_data = (x_test, y_test), epochs = 15, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d9e2c0",
   "metadata": {},
   "source": [
    "Total accuracy on training dataset = 96.36%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e372cc",
   "metadata": {},
   "source": [
    "### Model Evaluation on Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bd190ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Evaluating on testing data\n",
    "from sklearn.metrics import accuracy_score\n",
    "test = pd.read_csv(r\"D:\\Deep Learning\\Traffic Sign Identification\\data\\Test.csv\")\n",
    "test_img_path = \"D:\\Deep Learning\\Traffic Sign Identification\\data\"\n",
    "test_imgs = test['Path'].values\n",
    "test_data = []\n",
    "test_labels = []\n",
    "for img in test_imgs:\n",
    "    im = Image.open(test_img_path + '/' + img)\n",
    "    im = im.resize((30,30))\n",
    "    im = np.array(im)\n",
    "    test_data.append(im)\n",
    "test_data = np.array(test_data)\n",
    "predictions = cnn.predict_classes(test_data) #Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2adaa9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16  1 38 ...  6  7 10]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1fb64274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score #To check the accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "79c447d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = test['ClassId'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f06c0e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16,  1, 38, ...,  6,  7, 10], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "505f3ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9620744259699129"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_labels, predictions) #To predict accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca1e3b9",
   "metadata": {},
   "source": [
    "Total accuracy on testing dataset = 96.20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a4184c",
   "metadata": {},
   "source": [
    "Our model looks pretty solid because both the datasets accuracy are pretty close to each other which converges upto approx 96%\n",
    "\n",
    "Training accuracy = 96.36%\n",
    "\n",
    "Testing accuracy = 96.20%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c9d01",
   "metadata": {},
   "source": [
    "### Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d95e1e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "model_json = cnn.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# serialize weights to HDF5\n",
    "cnn.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd619c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ccf5ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413030d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1ac918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2658b874",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
